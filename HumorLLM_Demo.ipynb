{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HumorLLM: A Transformer-Based Language Model for Humor Generation\n",
    "\n",
    "This project implements a custom transformer architecture called **Seagull** for generating humorous captions and text. The model uses modern techniques including RoPE positional encoding, RMS normalization, and SwiGLU activation functions.\n",
    "\n",
    "## üöÄ Key Features\n",
    "\n",
    "- **Custom Transformer Architecture**: Seagull transformer with 12 layers, 768 embedding dimensions\n",
    "- **Modern Techniques**: RoPE, RMS LayerNorm, SwiGLU FFN, Gradient Clipping\n",
    "- **Optimized Training**: Mixed precision training, model compilation, cosine LR scheduling\n",
    "- **Humor-Focused**: Trained specifically on caption data for humor generation\n",
    "\n",
    "## üìä Model Performance\n",
    "\n",
    "**Best Configuration**: batch_size=16, epochs=2\n",
    "- **Validation Loss**: 2.559\n",
    "- **Perplexity**: 13.239\n",
    "- **Parameters**: ~85M parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"üîß PyTorch Version: {torch.__version__}\")\n",
    "print(f\"üîß CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üîß GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configuration\n",
    "with open('config/model_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"üèóÔ∏è Model Configuration:\")\n",
    "for section, params in config.items():\n",
    "    print(f\"\\n{section.upper()}:\")\n",
    "    if isinstance(params, dict):\n",
    "        for key, value in params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Seagull model\n",
    "from seagull.model.heads.seagull_lm import SeagullLM\n",
    "from seagull.data_processing.bbpe import BBPE\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BBPE()\n",
    "tokenizer.load_state_dict(torch.load('tokenizer/state_dict.json', map_location='cpu'))\n",
    "\n",
    "print(f\"üìù Tokenizer Vocabulary Size: {tokenizer.vocab_size}\")\n",
    "print(f\"üìù Special Tokens: BOS={tokenizer.bos_token_id}, EOS={tokenizer.eos_token_id}, PAD={tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with config\n",
    "model_config = config['model']\n",
    "model_config['vocab_size'] = tokenizer.vocab_size\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = SeagullLM(**model_config)\n",
    "\n",
    "# Load trained weights\n",
    "try:\n",
    "    model_state = torch.load('models/final_model.pt', map_location=device)\n",
    "    model.load_state_dict(model_state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Model file not found. Please ensure models/final_model.pt exists.\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üî¢ Total Parameters: {total_params:,}\")\n",
    "print(f\"üî¢ Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"üî¢ Model Size: ~{total_params/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Humor Generation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_humor(prompt, max_length=50, temperature=0.8, top_k=50, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate humorous text continuation from a prompt\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        max_length: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (higher = more creative)\n",
    "        top_k: Top-k sampling\n",
    "        top_p: Nucleus sampling\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], device=device)\n",
    "    \n",
    "    generated_ids = input_ids.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get model predictions\n",
    "            outputs = model(torch.tensor([generated_ids], device=device))\n",
    "            logits = outputs.logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                logits = torch.full_like(logits, float('-inf'))\n",
    "                logits[top_k_indices] = top_k_logits\n",
    "            \n",
    "            # Apply top-p filtering\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # Remove tokens with cumulative probability above the threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "                sorted_indices_to_remove[0] = 0\n",
    "                \n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            # Stop if EOS token\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "            generated_ids.append(next_token)\n",
    "    \n",
    "    # Decode generated text\n",
    "    generated_text = tokenizer.decode(generated_ids)\n",
    "    return generated_text\n",
    "\n",
    "print(\"üé≠ Humor Generation Function Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo prompts for humor generation\n",
    "demo_prompts = [\n",
    "    \"A cat walks into a bar and\",\n",
    "    \"Why did the programmer quit his job?\",\n",
    "    \"The funniest thing about artificial intelligence is\",\n",
    "    \"My computer is so slow that\",\n",
    "    \"A robot, a human, and a cat are in an elevator when\"\n",
    "]\n",
    "\n",
    "print(\"üé™ Generating Humorous Completions...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(demo_prompts, 1):\n",
    "    print(f\"\\nüéØ Prompt {i}: {prompt}\")\n",
    "    \n",
    "    try:\n",
    "        # Generate multiple completions with different temperatures\n",
    "        for temp_name, temp_val in [(\"Conservative\", 0.6), (\"Balanced\", 0.8), (\"Creative\", 1.0)]:\n",
    "            completion = generate_humor(prompt, max_length=30, temperature=temp_val)\n",
    "            print(f\"\\n  {temp_name} (T={temp_val}): {completion}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error generating completion: {e}\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Humor Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive humor generation\n",
    "print(\"üéÆ Interactive Humor Generator\")\n",
    "print(\"Enter your prompts below (type 'quit' to exit)\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_prompt = input(\"üéØ Your prompt: \")\n",
    "        \n",
    "        if user_prompt.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Thanks for using HumorLLM!\")\n",
    "            break\n",
    "        \n",
    "        if user_prompt.strip():\n",
    "            print(\"\\nüé≠ Generating humor...\")\n",
    "            completion = generate_humor(user_prompt, max_length=40, temperature=0.8)\n",
    "            print(f\"\\n‚ú® Result: {completion}\\n\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüëã Goodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Model Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model architecture\n",
    "print(\"üîç Model Architecture Analysis\\n\")\n",
    "\n",
    "def analyze_layer(name, module, depth=0):\n",
    "    indent = \"  \" * depth\n",
    "    params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    print(f\"{indent}{name}: {type(module).__name__} ({params:,} parameters)\")\n",
    "\n",
    "print(\"üìã Layer-by-layer breakdown:\")\n",
    "for name, module in model.named_children():\n",
    "    analyze_layer(name, module)\n",
    "    \n",
    "    # Show transformer layers detail\n",
    "    if hasattr(module, 'layers') and name == 'transformer':\n",
    "        print(f\"  ‚Ü≥ {len(module.layers)} transformer layers\")\n",
    "        if len(module.layers) > 0:\n",
    "            layer_params = sum(p.numel() for p in module.layers[0].parameters())\n",
    "            print(f\"  ‚Ü≥ Each layer: ~{layer_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics visualization (if available)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # This would show training curves if training logs were available\n",
    "    print(\"üìà Training Metrics Summary:\")\n",
    "    print(f\"  Final Validation Loss: 2.559\")\n",
    "    print(f\"  Final Perplexity: 13.239\")\n",
    "    print(f\"  Training Configuration: batch_size=16, epochs=2\")\n",
    "    print(f\"  Optimizer: AdamW with cosine LR scheduling\")\n",
    "    print(f\"  Special Features: Mixed precision, gradient clipping, model compilation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"üìä Training visualization not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark generation speed\n",
    "print(\"‚ö° Performance Benchmarks\\n\")\n",
    "\n",
    "test_prompt = \"The funniest thing about\"\n",
    "num_runs = 5\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "\n",
    "print(f\"üß™ Running {num_runs} generation tests...\")\n",
    "\n",
    "for i in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    result = generate_humor(test_prompt, max_length=20, temperature=0.8)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    generation_time = end_time - start_time\n",
    "    tokens_generated = len(tokenizer.encode(result)) - len(tokenizer.encode(test_prompt))\n",
    "    \n",
    "    total_time += generation_time\n",
    "    total_tokens += tokens_generated\n",
    "    \n",
    "    print(f\"  Run {i+1}: {generation_time:.3f}s, {tokens_generated} tokens\")\n",
    "\n",
    "avg_time = total_time / num_runs\n",
    "avg_tokens = total_tokens / num_runs\n",
    "tokens_per_second = avg_tokens / avg_time if avg_time > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Average Results:\")\n",
    "print(f\"  Time per generation: {avg_time:.3f}s\")\n",
    "print(f\"  Tokens per generation: {avg_tokens:.1f}\")\n",
    "print(f\"  Generation speed: {tokens_per_second:.1f} tokens/second\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Project Structure and Usage\n",
    "\n",
    "```\n",
    "HumorLLM/\n",
    "‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ final_model.pt          # Trained model weights\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ final_checkpoint.ckpt   # Training checkpoint\n",
    "‚îú‚îÄ‚îÄ config/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ model_config.json       # Model configuration\n",
    "‚îú‚îÄ‚îÄ tokenizer/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json          # Tokenizer vocabulary\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ state_dict.json         # Tokenizer state\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ processed/              # Training datasets\n",
    "‚îú‚îÄ‚îÄ seagull/                    # Model architecture\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ model/                  # Core model components\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ nn/                     # Neural network modules\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ trainers/               # Training utilities\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils/                  # Helper functions\n",
    "‚îú‚îÄ‚îÄ scripts/                    # Training and utility scripts\n",
    "‚îî‚îÄ‚îÄ requirements.txt           # Dependencies\n",
    "```\n",
    "\n",
    "## üéØ Key Achievements\n",
    "\n",
    "1. **Custom Architecture**: Implemented modern transformer with RoPE, RMS LayerNorm, SwiGLU\n",
    "2. **Optimized Training**: Mixed precision, gradient clipping, cosine scheduling\n",
    "3. **Humor Focus**: Specialized training on caption data for humor generation\n",
    "4. **Production Ready**: Clean codebase with modular design and comprehensive configs\n",
    "\n",
    "## üîÆ Future Improvements\n",
    "\n",
    "- **Larger Scale**: Train on more diverse humor datasets\n",
    "- **Fine-tuning**: Domain-specific humor adaptation\n",
    "- **Evaluation**: Implement humor-specific metrics\n",
    "- **Deployment**: API endpoint for real-time humor generation\n",
    "\n",
    "---\n",
    "\n",
    "**Built with ‚ù§Ô∏è using PyTorch and custom Seagull architecture**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
